---
layout: post
title:  "Peeling LLMs"
date:   2025-03-04 01:25:00 +0530
categories: ml ai llm
author: "Jatin Katyal"
---

### **ğŸš€ Layers in a Large Language Model (LLM) â€“ A Deep Dive**  

LLMs like GPT, DeepSeek, and LLaMA consist of multiple layers that process text sequentially, refining the information at each stage. These layers can be categorized as follows:  

---

# **1ï¸âƒ£ Tokenization Layer (Input Representation)**
### **ğŸ”¹ Function**: Converts raw text into numerical representations (tokens).  
### **ğŸ”¹ Components**:
âœ… **Byte-Pair Encoding (BPE)** / **Unigram Tokenization** â€“ Splits words into subwords.  
âœ… **SentencePiece** â€“ A variant that handles out-of-vocabulary words better.  
âœ… **Embedding Lookup** â€“ Maps each token to a high-dimensional vector.  

### **ğŸ” Deep Dive**:
- GPT uses **BPE**, while **LLaMA and DeepSeek** use **SentencePiece**.  
- Embeddings capture word relationships based on pretraining data.  

ğŸ“Œ **Example**:  
"Transformer models are amazing" âŸ¶ `["Transform", "er", "models", "are", "amazing"]`  
Each token gets converted into a **vector of floating-point numbers**.

---

# **2ï¸âƒ£ Embedding Layer**
### **ğŸ”¹ Function**: Converts discrete tokens into dense, continuous vector representations.  
### **ğŸ”¹ Components**:
âœ… **Word Embeddings** â€“ Each token gets a unique vector from a pre-trained table.  
âœ… **Position Embeddings** â€“ Adds sequence order information.  
âœ… **Rotary Position Embeddings (RoPE)** â€“ Alternative method for relative positioning.  

### **ğŸ” Deep Dive**:
- **Regular embeddings** (GPT-3) use fixed **position encoding** (sinusoids).  
- **RoPE embeddings** (LLaMA, DeepSeek) use a rotation-based approach for better generalization in long contexts.  

ğŸ“Œ **Mathematical Formula** (for sin-cos encoding):  
$$PE(pos, 2i) = \sin(pos / 10000^{2i/d})$$
$$
PE(pos, 2i+1) = \cos(pos / 10000^{2i/d})
$$

ğŸ“Œ **RoPE formula** (Rotation Matrix):  
$$
RoPE(x) = x \cos(\theta) + R(x) \sin(\theta)
$$
where $$ R(x) $$ is a shifted version of $$ x $$.

ğŸ” **Why RoPE?** It enables models to extrapolate better to **longer sequences**.

---

# **3ï¸âƒ£ Self-Attention Layer (Core of Transformers)**
### **ğŸ”¹ Function**: Allows the model to **focus on important words** in a sentence.  
### **ğŸ”¹ Components**:
âœ… **Query (Q), Key (K), and Value (V) Matrices** â€“ Compute attention weights.  
âœ… **Scaled Dot-Product Attention** â€“ Assigns importance scores to tokens.  
âœ… **Multi-Head Attention (MHA)** â€“ Runs multiple attention mechanisms in parallel.  

### **ğŸ” Deep Dive**:
- **Formula for attention weights**:
  $$
  \text{Attention}(Q, K, V) = \text{softmax} \left(\frac{QK^T}{\sqrt{d_k}}\right) V
  $$
- **Why Scale by $$\sqrt{d_k}$$?**  
  Prevents values from becoming too large, stabilizing training.  

### **Multi-Head Attention**
- Instead of a **single attention mechanism**, we use **multiple heads** that learn different aspects of relationships.  

ğŸ“Œ **Example**:  
- One head focuses on **syntax** ("is" related to "running").  
- Another head focuses on **meaning** ("fast" relates to "speed").  

---

# **4ï¸âƒ£ Feedforward Layer (MLP Block)**
### **ğŸ”¹ Function**: Applies **non-linear transformations** to enhance representation learning.  
### **ğŸ”¹ Components**:
âœ… **Two linear layers (FC1 & FC2)** â€“ Transform and scale features.  
âœ… **Activation function (ReLU/GELU)** â€“ Adds non-linearity.  

### **ğŸ” Deep Dive**:
The **standard feedforward block**:
$$
\text{FFN}(x) = \text{ReLU}(W_1 x + b_1) W_2 + b_2
$$
where:  
- $$ W_1 $$, $$ W_2 $$ are weight matrices.  
- $$ b_1 $$, $$ b_2 $$ are bias terms.  

ğŸ” **Why GELU Instead of ReLU?**  
- **GELU** (used in GPT) improves smoothness and model performance.  
- GELU formula:
  $$
  \text{GELU}(x) = 0.5x (1 + \tanh(\sqrt{2/\pi} (x + 0.044715 x^3)))
  $$

---

# **5ï¸âƒ£ Normalization Layers**
### **ğŸ”¹ Function**: Stabilizes training and prevents overfitting.  
### **ğŸ”¹ Types**:
âœ… **Layer Normalization (LayerNorm)** â€“ Normalizes across feature dimensions.  
âœ… **RMS Normalization (RMSNorm)** â€“ Alternative used in DeepSeek models.  

### **ğŸ” Deep Dive**:
**LayerNorm Formula**:
$$
LN(x) = \frac{x - \mu}{\sigma} \gamma + \beta
$$
- **$$ \mu $$ and $$ \sigma $$** are mean and std of activations.  
- **$$ \gamma $$, $$ \beta $$** are learnable parameters.  

ğŸ“Œ **Why RMSNorm?**  
- Removes **mean centering**, making computation **more efficient**.

---

# **6ï¸âƒ£ Residual Connections**
### **ğŸ”¹ Function**: Helps deep models learn by skipping connections between layers.  
### **ğŸ” Deep Dive**:
Instead of passing data **sequentially**, residual layers **add** the input back to the output:  
$$\text{Output} = x + \text{Transform}(x)$$

ğŸ“Œ **Why?**  
- Helps gradients flow better (solves vanishing gradients).  
- Allows **deep networks** to train **without degradation**.

---

# **7ï¸âƒ£ Output Layer (Final Prediction)**
### **ğŸ”¹ Function**: Converts processed representations into a probability distribution over vocabulary tokens.  
### **ğŸ”¹ Components**:
âœ… **Final Linear Layer** â€“ Maps to vocab size.  
âœ… **Softmax Activation** â€“ Converts logits into probabilities.  

### **ğŸ” Deep Dive**:
$$P(\text{word} | \text{context}) = \frac{e^{z_i}}{\sum_j e^{z_j}}$$
where $$ z_i $$ is the logit for word $$ i $$.

ğŸ“Œ **Why Softmax?**  
- Converts raw scores into **probabilities** that sum to **1**.

---

# **ğŸš€ Special Layers in LLM Variants**
### **ğŸ”¹ Rotary Position Embeddings (RoPE)**
âœ… **Used in LLaMA, DeepSeek** for better **long-context learning**.  

### **ğŸ”¹ Gated Linear Units (GLU)**
âœ… **Used in DeepSeek** to improve **transformer efficiency**.  

### **ğŸ”¹ SwiGLU Activation**
âœ… **Used in GPT-4** to improve training dynamics.  

---

# **ğŸ”¬ Summary: How These Layers Work Together**
1ï¸âƒ£ **Tokenization Layer** â€“ Converts text to tokens.  
2ï¸âƒ£ **Embedding Layer** â€“ Converts tokens to vectors.  
3ï¸âƒ£ **Self-Attention Layer** â€“ Learns relationships between words.  
4ï¸âƒ£ **Feedforward Layer** â€“ Applies transformations.  
5ï¸âƒ£ **Normalization Layers** â€“ Stabilizes training.  
6ï¸âƒ£ **Residual Connections** â€“ Helps deep learning.  
7ï¸âƒ£ **Output Layer** â€“ Generates predictions.  

ğŸ“Œ **All these layers stack to form a deep transformer model!**  
